{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchcontrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch, Sklearn imports\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "## Embeddings\n",
    "import allennlp\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "# Torch, Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rsilvei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP libs\n",
    "from nltk import download\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: AddToPlaylist, Length: 300\n",
      "Intent: BookRestaurant, Length: 300\n",
      "Intent: GetWeather, Length: 300\n",
      "Intent: PlayMusic, Length: 300\n",
      "Intent: RateBook, Length: 300\n",
      "Intent: SearchCreativeWork, Length: 300\n",
      "Intent: SearchScreeningEvent, Length: 300\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns = ['phrase', 'intent'])\n",
    "for intent in ['AddToPlaylist', 'BookRestaurant', 'GetWeather', 'PlayMusic', 'RateBook', 'SearchCreativeWork',\n",
    "               'SearchScreeningEvent']:\n",
    "    with open(\"./2017-06-custom-intent-engines/\" + intent + \"/train_\" + intent + \".json\",\n",
    "              encoding='cp1251') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    print(\"Intent: {}, Length: {}\".format(intent,len(data[intent])))\n",
    "    texts = []\n",
    "    for i in range(len(data[intent])):\n",
    "        text = ''\n",
    "        for j in range(len(data[intent][i]['data'])):\n",
    "            text += data[intent][i]['data'][j]['text']\n",
    "        dataset = dataset.append({'phrase': text, 'intent': intent}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text, do_stop=False, do_stem=False):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Cleaning input\n",
    "    text = text.replace(\"'s\",\"\")\n",
    "    text = text.replace(\"â€™s\",\"\")\n",
    "    text = text.replace(\"?\",\"\")\n",
    "    text = text.replace(\"-\",\"\")\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    # Removing all the stopwords\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    if (do_stem==True):\n",
    "        # Stemming\n",
    "        text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    return text\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in PUNCT)\n",
    "\n",
    "## Lemmatization function based on Spacy Library\n",
    "def lemmatizer_spacy(text):        \n",
    "    sent = []\n",
    "    doc = spacy_en(text)\n",
    "    for word in doc:\n",
    "        if word.lemma_ == \"-PRON-\":\n",
    "            sent.append(word.text)\n",
    "        else:\n",
    "            sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['preproc_text'] = dataset['phrase'].apply(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build label vocabulary\n",
    "label_to_ix = {}\n",
    "for label in dataset.intent:\n",
    "    for word in label.split():\n",
    "        if word not in label_to_ix:\n",
    "            label_to_ix[word]=len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intents(Dataset):\n",
    "    def __init__(self, dataframe, w2v_weights_path):\n",
    "        self.len = len(dataframe)\n",
    "        self.label_to_ix = {}\n",
    "        self.data = dataframe\n",
    "        self.w2v = w2v = KeyedVectors.load_word2vec_format(w2v_weights_path, binary = True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        phrase = self.data.preproc_text[index]\n",
    "        X, _  = self.get_avg_sentence_vector(phrase)\n",
    "        y = label_to_ix[self.data.intent[index]]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def get_avg_sentence_vector(self, sentence):\n",
    "        featureVec = np.zeros((self.w2v.vector_size), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "        not_found_words = []\n",
    "        for word in sentence.split():\n",
    "            if word in self.w2v.index2word:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, self.w2v.get_vector(word))\n",
    "            else:\n",
    "                not_found_words.append(word)\n",
    "        if nwords>0:\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec, not_found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data locations for embeddings\n",
    "elmo_config_key_path = '../../../vectors/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
    "elmo_weights_key_path = '../../../vectors/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
    "w2v_weights_path = '../../../vectors/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin\r\n",
      "crawl-300d-2M.vec\r\n",
      "\u001b[31melmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\u001b[m\u001b[m\r\n",
      "\u001b[31melmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\u001b[m\u001b[m\r\n",
      "glove.42B.300d.txt\r\n",
      "glove.840B.300d.txt\r\n",
      "lid.176.ftz\r\n",
      "\u001b[1m\u001b[36mprogram\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36muncased_L-12_H-768_A-12\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36muncased_L-24_H-1024_A-16\u001b[m\u001b[m\r\n",
      "wiki-news-300d-1M-subword.txt\r\n",
      "wiki-news-300d-1M.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2100, 3)\n",
      "TRAIN Dataset: (1680, 3)\n",
      "TEST Dataset: (420, 3)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=dataset.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
    "test_dataset=dataset.drop(train_dataset.index).reset_index(drop=True)\n",
    "print(\"FULL Dataset: {}\".format(dataset.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Intents(train_dataset,  w2v_weights_path)\n",
    "testing_set = Intents(test_dataset, w2v_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, inputdim, \n",
    "                        nclasses, \n",
    "                        nhidden, \n",
    "                        dropout = 0,\n",
    "                        cudaEfficient=True):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "        -dropout:    dropout for MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        self.inputdim = inputdim\n",
    "        self.hidden_dim = nhidden\n",
    "        self.dropout = dropout\n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        if cudaEfficient:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, nhidden),\n",
    "                nn.Dropout(p=self.dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(nhidden, self.nclasses),\n",
    "                ).cuda()\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, nhidden),\n",
    "                nn.Dropout(p=self.dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(nhidden, self.nclasses),\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        log_probs = self.model(x)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIM = training_set.w2v.vector_size\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "NHIDDEN = 512\n",
    "DROPOUT = 0.2\n",
    "model = SimpleMLP(inputdim = INP_DIM ,\n",
    "          nhidden = NHIDDEN,\n",
    "          nclasses = NUM_LABELS,\n",
    "          dropout = DROPOUT, \n",
    "          cudaEfficient = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# params = {'batch_size': 64,\n",
    "#           'shuffle': True,\n",
    "#           'num_workers': 1}\n",
    "# training_loader = DataLoader(training_set, **params)\n",
    "# testing_loader = DataLoader(testing_set, **params)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epochs = 5\n",
    "# for epoch in range(max_epochs):\n",
    "#     print(\"EPOCH -- {}\".format(epoch))\n",
    "#     for i, (sent, label) in enumerate(training_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         sent = Variable(sent)\n",
    "#         label = Variable(label)\n",
    "#         output = model.forward(sent)\n",
    "#         _, predicted = torch.max(output.data, 1)\n",
    "#         loss = loss_function(output, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if i%100 == 0:\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             for sent, label in testing_loader:\n",
    "#                 sent = Variable(sent)\n",
    "#                 label = Variable(label)\n",
    "#                 output = model.forward(sent)\n",
    "#                 _, predicted = torch.max(output.data, 1)\n",
    "#                 total += label.size(0)\n",
    "#                 correct += (predicted.cpu() == label.cpu()).sum()\n",
    "#             accuracy = 100.00 * correct.numpy() / total\n",
    "#             # Print Loss\n",
    "#             print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reply(phrase, model):\n",
    "    inp, _ = training_set.get_avg_sentence_vector(phrase)\n",
    "    inp = Variable(torch.Tensor(inp))\n",
    "    output = model.forward(inp)\n",
    "    _, predicted = torch.max(output.data, 0)\n",
    "    pred_label=list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted.item())]\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RateBook'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"i need to book a restaurant today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SearchScreeningEvent'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"play music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SearchScreeningEvent'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"Obama is cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stochastic Weight Averaging (SWA) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "training_loader = DataLoader(training_set, **params)\n",
    "testing_loader = DataLoader(testing_set, **params)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "base_opt = optim.Adam(params =  model.parameters(), lr=learning_rate)\n",
    "opt = SWA(base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH -- 0\n",
      "Iteration: 0. Loss: 1.952578067779541. Accuracy: 51.666666666666664%\n",
      "EPOCH -- 1\n",
      "Iteration: 0. Loss: 0.18529416620731354. Accuracy: 92.85714285714286%\n",
      "EPOCH -- 2\n",
      "Iteration: 0. Loss: 0.09568418562412262. Accuracy: 97.61904761904762%\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"EPOCH -- {}\".format(epoch))\n",
    "    for i, (sent, label) in enumerate(training_loader):\n",
    "        opt.zero_grad()\n",
    "        sent = Variable(sent)\n",
    "        label = Variable(label)\n",
    "        output = model.forward(sent)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i%100 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for sent, label in testing_loader:\n",
    "                sent = Variable(sent)\n",
    "                label = Variable(label)\n",
    "                output = model.forward(sent)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted.cpu() == label.cpu()).sum()\n",
    "            accuracy = 100.00 * correct.numpy() / total\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))\n",
    "opt.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_elmo_swag_uncertainty.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BookRestaurant'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"i need to book a restaurant today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlayMusic'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"play music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GetWeather'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"Obama is cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 987120\r\n",
      "drwxr-xr-x  10 rsilvei  ADESI\\Domain Users   320B Aug 30 10:01 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  24 rsilvei  ADESI\\Domain Users   768B Aug 21 10:43 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   4 rsilvei  ADESI\\Domain Users   128B Aug 26 10:41 \u001b[1m\u001b[36m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 rsilvei  ADESI\\Domain Users    30K Aug 19 15:14 1.text_classifier_roberta.ipynb\r\n",
      "-rw-r--r--   1 rsilvei  ADESI\\Domain Users    20K Aug 30 10:01 2.uncertainty_swag.ipynb\r\n",
      "drwxr-xr-x@ 10 rsilvei  ADESI\\Domain Users   320B Aug 19 09:36 \u001b[1m\u001b[36m2017-06-custom-intent-engines\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 rsilvei  ADESI\\Domain Users   617K Aug 30 09:58 model_elmo_swag_uncertainty.pth\r\n",
      "-rw-r--r--@  1 rsilvei  ADESI\\Domain Users   446K Aug 19 15:10 roberta-base-merges.txt\r\n",
      "-rw-r--r--@  1 rsilvei  ADESI\\Domain Users   478M Aug 19 15:42 roberta-base-pytorch_model.bin\r\n",
      "-rw-r--r--@  1 rsilvei  ADESI\\Domain Users   878K Aug 19 15:13 roberta-base-vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelito = SimpleMLP(inputdim = INP_DIM ,\n",
    "          nhidden = NHIDDEN,\n",
    "          nclasses = NUM_LABELS,\n",
    "          dropout = DROPOUT, \n",
    "          cudaEfficient = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelito.load_state_dict(torch.load('model_elmo_swag_uncertainty.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GetWeather'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"Obama is cool\", modelito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlayMusic'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"play music\", modelito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BookRestaurant'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reply(\"i need to book a restaurant today\", modelito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'swag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-eee7dcf3e26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mswag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'swag'"
     ]
    }
   ],
   "source": [
    "import swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_new)",
   "language": "python",
   "name": "nlp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
