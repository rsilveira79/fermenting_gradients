{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "- Weighed sum over input sequence\n",
    "\n",
    "$$\n",
    "y_{i} = \\sum_{j} w_{ij} x_{j}\n",
    "$$\n",
    "\n",
    "- $ w_{ij}$ is not a weight, it is derived from a function over $x_{i}$ and $x_{j}$\n",
    "\n",
    "$$\n",
    "w'_{ij} = x_{i}^{T}x_{j}\n",
    "$$\n",
    "\n",
    "Apply softmax in $w'_{ij}$, so ensuring values sum up to 1:\n",
    "$$\n",
    "w_{ij} = \\frac{\\exp{w'_{ij}}}{\\sum_{j} \\exp{w'_{ij}}}\n",
    "$$\n",
    "\n",
    "<img src=\"tmp/self-attention.svg\"  width=\"512\" height=\"512\" align=\"center\"/>\n",
    "\n",
    "- **Self Attention** is the fundamental operation. This is only operation that is applied **between** vectors  \n",
    "- Self Attention sees its inputs as a _set_, not a sequence. It _ignores_ sequential nature of the input (if we permute items, the output will be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_attention(x):\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "    y = torch.bmm(weights, x)\n",
    "    return y, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lenght = 10\n",
    "x = torch.rand([1,seq_lenght,10])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, w = get_self_attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query, Keys, Values\n",
    "\n",
    "- Every input $x_{i}$ does basically 3 things:\n",
    " - Compare to very other vector to establish weights for own output $y_{i}$ \n",
    " - Compare to very other vector to establish weights for the j-th output $y_{j}$ \n",
    " - Used as weighted sum to compute output vector once weights are established \n",
    " \n",
    " $$ q_{i} = W_{q} \\cdot x_{i} $$\n",
    " $$ k_{i} = W_{k} \\cdot x_{i} $$\n",
    " $$ v_{i} = W_{v} \\cdot x_{i} $$\n",
    " \n",
    " \n",
    "$$ w_{ij}' = q_{i}^T \\cdot k_{j} $$\n",
    "$$ w_{ij} = softmax(w_{ij}') $$\n",
    "\n",
    "$$ y_{i} = \\sum_{j} w_{ij} \\cdot v_{j} $$\n",
    "\n",
    "\n",
    "The general mechanism was as follows:\n",
    " - Inputs $\\rightarrow$ We call the input the **values**\n",
    " - Some (trainable) mechanism assigns a **key** to each value. \n",
    " - Then to each output, some other mechanism assigns a **query**.\n",
    " \n",
    "The great breakthrough of self-attention was that attention by itself is a strong enough mechanism to do all the learning.\n",
    "Attention is all you need, as the authors put it. \n",
    "The key, query and value are all the same vectors (with minor linear transformations). \n",
    "They attend to themselves and stacking such self-attention provides sufficient nonlinearity and representational power to learn very complicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWide(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values  = self.tovalues(x) .view(b, t, h, e)\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionNarrow(nn.Module):\n",
    "\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        s = emb // heads\n",
    "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
    "\n",
    "        self.tokeys    = nn.Linear(s, s, bias=False)\n",
    "        self.toqueries = nn.Linear(s, s, bias=False)\n",
    "        self.tovalues  = nn.Linear(s, s, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * s, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "        s = e // h\n",
    "        x = x.view(b, t, h, s)\n",
    "\n",
    "        keys    = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values  = self.tovalues(x)\n",
    "\n",
    "        assert keys.size() == (b, t, h, s)\n",
    "        assert queries.size() == (b, t, h, s)\n",
    "        assert values.size() == (b, t, h, s)\n",
    "\n",
    "        # Compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 10\n",
    "att = SelfAttentionWide(emb = emb_dim, heads= 8)\n",
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "x = torch.rand([batch_size, emb_dim, emb_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,w = att.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Head 1\n",
    "plt.imshow(w[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Head 2\n",
    "plt.imshow(w[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Head 3\n",
    "plt.imshow(w[2].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/transformer-block.svg\"  width=\"512\" height=\"512\" align=\"center\"/>\n",
    "\n",
    "- The order of the various components is not set in stone\n",
    "- The important thing is to combine self-attention with a local feedforward, and to add normalization and residual connections\n",
    "- Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately\n",
    "- The layer normalization is applied over the embedding dimension only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttentionWide(emb, heads=heads, mask=mask) if wide \\\n",
    "                    else SelfAttentionNarrow(emb, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        feedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(feedforward + x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = TransformerBlock(emb_dim, 4, mask = False, seq_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for classifying sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout, wide=wide))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "\n",
    "        x = self.toprobs(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for generating text (character by character).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, wide=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=True, wide=wide))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (batch, sequence length) integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = self.toprobs(x.view(b*t, e)).view(b, t, self.num_tokens)\n",
    "\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/encoder-decoder.svg\"  width=\"512\" height=\"512\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Teacher forcing refers to the technique of also allowing the decoder access to the input sentence, but in an autoregressive fashion.  \n",
    "That is, the decoder generates the output sentence word for word based both on the latent vector and the words it has already generated.  \n",
    "This takes some of the pressure off the latent representation: the decoder can use word-for-word sampling to take care of the low-level structure like syntax and grammar and use the latent vector to capture more high-level semantic structure.  \n",
    "Decoding twice with the same latent vector would, ideally, give you two different sentences with the same meaning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.4', '1.5.1')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<eos>',\n",
       " '=',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'iii',\n",
       " '=',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " 'senjō',\n",
       " 'no',\n",
       " 'valkyria',\n",
       " '3',\n",
       " '<unk>',\n",
       " 'chronicles',\n",
       " '(',\n",
       " 'japanese',\n",
       " '戦場のヴァルキュリア3',\n",
       " ',',\n",
       " 'lit',\n",
       " '.',\n",
       " 'valkyria',\n",
       " 'of',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " '3',\n",
       " ')',\n",
       " ',',\n",
       " 'commonly',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'iii',\n",
       " 'outside',\n",
       " 'japan',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'tactical',\n",
       " 'role',\n",
       " '@-@',\n",
       " 'playing',\n",
       " 'video',\n",
       " 'game',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'sega',\n",
       " 'and',\n",
       " 'media',\n",
       " '.',\n",
       " 'vision',\n",
       " 'for',\n",
       " 'the',\n",
       " 'playstation',\n",
       " 'portable',\n",
       " '.',\n",
       " 'released',\n",
       " 'in',\n",
       " 'january',\n",
       " '2011',\n",
       " 'in',\n",
       " 'japan',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'third',\n",
       " 'game',\n",
       " 'in',\n",
       " 'the',\n",
       " 'valkyria',\n",
       " 'series',\n",
       " '.',\n",
       " '<unk>',\n",
       " 'the',\n",
       " 'same',\n",
       " 'fusion',\n",
       " 'of',\n",
       " 'tactical',\n",
       " 'and',\n",
       " 'real',\n",
       " '@-@',\n",
       " 'time',\n",
       " 'gameplay',\n",
       " 'as',\n",
       " 'its',\n",
       " 'predecessors',\n",
       " ',',\n",
       " 'the',\n",
       " 'story',\n",
       " 'runs',\n",
       " 'parallel',\n",
       " 'to',\n",
       " 'the',\n",
       " 'first',\n",
       " 'game',\n",
       " 'and',\n",
       " 'follows',\n",
       " 'the',\n",
       " 'nameless',\n",
       " ',',\n",
       " 'a',\n",
       " 'penal',\n",
       " 'military',\n",
       " 'unit',\n",
       " 'serving',\n",
       " 'the',\n",
       " 'nation',\n",
       " 'of',\n",
       " 'gallia',\n",
       " 'during',\n",
       " 'the',\n",
       " 'second',\n",
       " 'europan',\n",
       " 'war',\n",
       " 'who',\n",
       " 'perform',\n",
       " 'secret',\n",
       " 'black',\n",
       " 'operations',\n",
       " 'and',\n",
       " 'are',\n",
       " 'pitted',\n",
       " 'against',\n",
       " 'the',\n",
       " 'imperial',\n",
       " 'unit',\n",
       " '<unk>',\n",
       " 'raven',\n",
       " '.',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'game',\n",
       " 'began',\n",
       " 'development',\n",
       " 'in',\n",
       " '2010',\n",
       " ',',\n",
       " 'carrying',\n",
       " 'over',\n",
       " 'a',\n",
       " 'large',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'work',\n",
       " 'done',\n",
       " 'on',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'ii',\n",
       " '.',\n",
       " 'while',\n",
       " 'it',\n",
       " 'retained',\n",
       " 'the',\n",
       " 'standard',\n",
       " 'features',\n",
       " 'of',\n",
       " 'the',\n",
       " 'series',\n",
       " ',',\n",
       " 'it',\n",
       " 'also',\n",
       " 'underwent',\n",
       " 'multiple',\n",
       " 'adjustments',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'making',\n",
       " 'the',\n",
       " 'game',\n",
       " 'more',\n",
       " '<unk>',\n",
       " 'for',\n",
       " 'series',\n",
       " 'newcomers',\n",
       " '.',\n",
       " 'character',\n",
       " 'designer',\n",
       " '<unk>',\n",
       " 'honjou',\n",
       " 'and',\n",
       " 'composer',\n",
       " 'hitoshi',\n",
       " 'sakimoto',\n",
       " 'both',\n",
       " 'returned',\n",
       " 'from',\n",
       " 'previous',\n",
       " 'entries',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'ii',\n",
       " 'director',\n",
       " 'takeshi',\n",
       " 'ozawa',\n",
       " '.',\n",
       " 'a',\n",
       " 'large',\n",
       " 'team',\n",
       " 'of',\n",
       " 'writers',\n",
       " 'handled',\n",
       " 'the',\n",
       " 'script',\n",
       " '.',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'\",\n",
       " 's',\n",
       " 'opening',\n",
       " 'theme',\n",
       " 'was',\n",
       " 'sung',\n",
       " 'by',\n",
       " 'may',\n",
       " \"'\",\n",
       " 'n',\n",
       " '.',\n",
       " '<eos>',\n",
       " 'it',\n",
       " 'met',\n",
       " 'with',\n",
       " 'positive',\n",
       " 'sales',\n",
       " 'in',\n",
       " 'japan',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'praised',\n",
       " 'by',\n",
       " 'both',\n",
       " 'japanese',\n",
       " 'and',\n",
       " 'western',\n",
       " 'critics',\n",
       " '.',\n",
       " 'after',\n",
       " 'release',\n",
       " ',',\n",
       " 'it',\n",
       " 'received',\n",
       " 'downloadable',\n",
       " 'content',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'an',\n",
       " 'expanded',\n",
       " 'edition',\n",
       " 'in',\n",
       " 'november',\n",
       " 'of',\n",
       " 'that',\n",
       " 'year',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'adapted',\n",
       " 'into',\n",
       " 'manga',\n",
       " 'and',\n",
       " 'an',\n",
       " 'original',\n",
       " 'video',\n",
       " 'animation',\n",
       " 'series',\n",
       " '.',\n",
       " 'due',\n",
       " 'to',\n",
       " 'low',\n",
       " 'sales',\n",
       " 'of',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'ii',\n",
       " ',',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'iii',\n",
       " 'was',\n",
       " 'not',\n",
       " 'localized',\n",
       " ',',\n",
       " 'but',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'translation',\n",
       " 'compatible',\n",
       " 'with',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'\",\n",
       " 's',\n",
       " 'expanded',\n",
       " 'edition',\n",
       " 'was',\n",
       " 'released',\n",
       " 'in',\n",
       " '2014',\n",
       " '.',\n",
       " 'media',\n",
       " '.',\n",
       " 'vision',\n",
       " 'would',\n",
       " 'return',\n",
       " 'to',\n",
       " 'the',\n",
       " 'franchise',\n",
       " 'with',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'valkyria',\n",
       " 'azure',\n",
       " 'revolution',\n",
       " 'for',\n",
       " 'the',\n",
       " 'playstation',\n",
       " '4',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '=',\n",
       " '=',\n",
       " 'gameplay',\n",
       " '=',\n",
       " '=',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " 'as',\n",
       " 'with',\n",
       " 'previous',\n",
       " '<unk>',\n",
       " 'chronicles',\n",
       " 'games',\n",
       " ',',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'iii',\n",
       " 'is',\n",
       " 'a',\n",
       " 'tactical',\n",
       " 'role',\n",
       " '@-@',\n",
       " 'playing',\n",
       " 'game',\n",
       " 'where',\n",
       " 'players',\n",
       " 'take',\n",
       " 'control',\n",
       " 'of',\n",
       " 'a',\n",
       " 'military',\n",
       " 'unit',\n",
       " 'and',\n",
       " 'take',\n",
       " 'part',\n",
       " 'in',\n",
       " 'missions',\n",
       " 'against',\n",
       " 'enemy',\n",
       " 'forces',\n",
       " '.',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'told',\n",
       " 'through',\n",
       " 'comic',\n",
       " 'book',\n",
       " '@-@',\n",
       " 'like',\n",
       " 'panels',\n",
       " 'with',\n",
       " 'animated',\n",
       " 'character',\n",
       " 'portraits',\n",
       " ',',\n",
       " 'with',\n",
       " 'characters',\n",
       " 'speaking',\n",
       " 'partially',\n",
       " 'through',\n",
       " 'voiced',\n",
       " 'speech',\n",
       " 'bubbles',\n",
       " 'and',\n",
       " 'partially',\n",
       " 'through',\n",
       " '<unk>',\n",
       " 'text',\n",
       " '.',\n",
       " 'the',\n",
       " 'player',\n",
       " 'progresses',\n",
       " 'through',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'linear',\n",
       " 'missions',\n",
       " ',',\n",
       " 'gradually',\n",
       " 'unlocked',\n",
       " 'as',\n",
       " 'maps',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'freely',\n",
       " '<unk>',\n",
       " 'through',\n",
       " 'and',\n",
       " 'replayed',\n",
       " 'as',\n",
       " 'they',\n",
       " 'are',\n",
       " 'unlocked',\n",
       " '.',\n",
       " 'the',\n",
       " 'route',\n",
       " 'to',\n",
       " 'each',\n",
       " 'story',\n",
       " 'location',\n",
       " 'on',\n",
       " 'the',\n",
       " 'map',\n",
       " 'varies',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'an',\n",
       " 'individual',\n",
       " 'player',\n",
       " \"'\",\n",
       " 's',\n",
       " 'approach',\n",
       " 'when',\n",
       " 'one',\n",
       " 'option',\n",
       " 'is',\n",
       " 'selected',\n",
       " ',',\n",
       " 'the',\n",
       " 'other',\n",
       " 'is',\n",
       " 'sealed',\n",
       " 'off',\n",
       " 'to',\n",
       " 'the',\n",
       " 'player',\n",
       " '.',\n",
       " 'outside',\n",
       " 'missions',\n",
       " ',',\n",
       " 'the',\n",
       " 'player',\n",
       " 'characters',\n",
       " 'rest',\n",
       " 'in',\n",
       " 'a',\n",
       " 'camp',\n",
       " ',',\n",
       " 'where',\n",
       " 'units',\n",
       " 'can',\n",
       " 'be',\n",
       " 'customized',\n",
       " 'and',\n",
       " 'character',\n",
       " 'growth',\n",
       " 'occurs',\n",
       " '.',\n",
       " 'alongside',\n",
       " 'the',\n",
       " 'main',\n",
       " 'story',\n",
       " 'missions',\n",
       " 'are',\n",
       " 'character',\n",
       " '@-@',\n",
       " 'specific',\n",
       " 'sub',\n",
       " 'missions',\n",
       " 'relating',\n",
       " 'to',\n",
       " 'different',\n",
       " 'squad',\n",
       " 'members',\n",
       " '.',\n",
       " 'after',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'\",\n",
       " 's',\n",
       " 'completion',\n",
       " ',',\n",
       " 'additional',\n",
       " 'episodes',\n",
       " 'are',\n",
       " 'unlocked',\n",
       " ',',\n",
       " 'some',\n",
       " 'of',\n",
       " 'them',\n",
       " 'having',\n",
       " 'a',\n",
       " 'higher',\n",
       " 'difficulty',\n",
       " 'than',\n",
       " 'those',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'game',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'also',\n",
       " 'love',\n",
       " 'simulation',\n",
       " 'elements',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'\",\n",
       " 's',\n",
       " 'two',\n",
       " 'main',\n",
       " '<unk>',\n",
       " ',',\n",
       " 'although',\n",
       " 'they',\n",
       " 'take',\n",
       " 'a',\n",
       " 'very',\n",
       " 'minor',\n",
       " 'role',\n",
       " '.',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'\",\n",
       " 's',\n",
       " 'battle',\n",
       " 'system',\n",
       " ',',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'system',\n",
       " ',',\n",
       " 'is',\n",
       " 'carried',\n",
       " 'over',\n",
       " 'directly',\n",
       " 'from',\n",
       " '<unk>',\n",
       " 'chronicles',\n",
       " '.',\n",
       " 'during',\n",
       " 'missions',\n",
       " ',',\n",
       " 'players',\n",
       " 'select',\n",
       " 'each',\n",
       " 'unit',\n",
       " 'using',\n",
       " 'a',\n",
       " 'top',\n",
       " '@-@',\n",
       " 'down',\n",
       " 'perspective',\n",
       " 'of',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " 'map',\n",
       " 'once',\n",
       " 'a',\n",
       " 'character',\n",
       " 'is',\n",
       " 'selected',\n",
       " ',',\n",
       " 'the',\n",
       " 'player',\n",
       " 'moves',\n",
       " 'the',\n",
       " 'character',\n",
       " 'around',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " 'in',\n",
       " 'third',\n",
       " '@-@',\n",
       " 'person',\n",
       " '.',\n",
       " 'a',\n",
       " 'character',\n",
       " 'can',\n",
       " 'only',\n",
       " 'act',\n",
       " 'once',\n",
       " 'per',\n",
       " '@-@',\n",
       " 'turn',\n",
       " ',',\n",
       " 'but',\n",
       " 'characters',\n",
       " 'can',\n",
       " 'be',\n",
       " 'granted',\n",
       " 'multiple',\n",
       " 'turns',\n",
       " 'at',\n",
       " 'the',\n",
       " 'expense',\n",
       " 'of',\n",
       " 'other',\n",
       " 'characters',\n",
       " \"'\",\n",
       " 'turns',\n",
       " '.',\n",
       " 'each',\n",
       " 'character',\n",
       " 'has',\n",
       " 'a',\n",
       " 'field',\n",
       " 'and',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'movement',\n",
       " 'limited',\n",
       " 'by',\n",
       " 'their',\n",
       " 'action',\n",
       " '<unk>',\n",
       " '.',\n",
       " 'up',\n",
       " 'to',\n",
       " 'nine',\n",
       " 'characters',\n",
       " 'can',\n",
       " 'be',\n",
       " 'assigned',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'mission',\n",
       " '.',\n",
       " 'during',\n",
       " 'gameplay',\n",
       " ',',\n",
       " 'characters',\n",
       " 'will',\n",
       " 'call',\n",
       " 'out',\n",
       " 'if',\n",
       " 'something',\n",
       " 'happens',\n",
       " 'to',\n",
       " 'them',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'their',\n",
       " 'health',\n",
       " 'points',\n",
       " '(',\n",
       " 'hp',\n",
       " ')',\n",
       " 'getting',\n",
       " 'low',\n",
       " 'or',\n",
       " 'being',\n",
       " 'knocked',\n",
       " 'out',\n",
       " 'by',\n",
       " 'enemy',\n",
       " 'attacks',\n",
       " '.',\n",
       " 'each',\n",
       " 'character',\n",
       " 'has',\n",
       " 'specific',\n",
       " 'potentials',\n",
       " ',',\n",
       " 'skills',\n",
       " 'unique',\n",
       " 'to',\n",
       " 'each',\n",
       " 'character',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'personal',\n",
       " 'potential',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'innate',\n",
       " 'skills',\n",
       " 'that',\n",
       " 'remain',\n",
       " 'unaltered',\n",
       " 'unless',\n",
       " 'otherwise',\n",
       " 'dictated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'story',\n",
       " 'and',\n",
       " 'can',\n",
       " 'either',\n",
       " 'help',\n",
       " 'or',\n",
       " 'impede',\n",
       " 'a',\n",
       " 'character',\n",
       " ',',\n",
       " 'and',\n",
       " 'battle',\n",
       " 'potentials',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'grown',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'game',\n",
       " 'and',\n",
       " 'always',\n",
       " 'grant',\n",
       " '<unk>',\n",
       " 'to',\n",
       " 'a',\n",
       " 'character',\n",
       " '.',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'battle',\n",
       " 'potentials',\n",
       " ',',\n",
       " 'each',\n",
       " 'character',\n",
       " 'has',\n",
       " 'a',\n",
       " 'unique',\n",
       " 'masters',\n",
       " 'table',\n",
       " ',',\n",
       " 'a',\n",
       " 'grid',\n",
       " '@-@',\n",
       " 'based',\n",
       " 'skill',\n",
       " 'table',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'acquire',\n",
       " 'and',\n",
       " 'link',\n",
       " 'different',\n",
       " 'skills',\n",
       " '.',\n",
       " 'characters',\n",
       " 'also',\n",
       " 'have',\n",
       " 'special',\n",
       " '<unk>',\n",
       " 'that',\n",
       " 'grant',\n",
       " 'them',\n",
       " 'temporary',\n",
       " '<unk>',\n",
       " 'on',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " 'kurt',\n",
       " 'can',\n",
       " 'activate',\n",
       " 'direct',\n",
       " 'command',\n",
       " 'and',\n",
       " 'move',\n",
       " 'around',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " 'without',\n",
       " '<unk>',\n",
       " 'his',\n",
       " 'action',\n",
       " 'point',\n",
       " 'gauge',\n",
       " ',',\n",
       " 'the',\n",
       " 'character',\n",
       " '<unk>',\n",
       " 'can',\n",
       " 'shift',\n",
       " 'into',\n",
       " 'her',\n",
       " 'valkyria',\n",
       " 'form',\n",
       " 'and',\n",
       " 'become',\n",
       " '<unk>',\n",
       " ',',\n",
       " 'while',\n",
       " 'imca',\n",
       " 'can',\n",
       " 'target',\n",
       " 'multiple',\n",
       " 'enemy',\n",
       " 'units',\n",
       " 'with',\n",
       " 'her',\n",
       " 'heavy',\n",
       " 'weapon',\n",
       " '.',\n",
       " '<eos>',\n",
       " 'troops',\n",
       " 'are',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'five',\n",
       " 'classes',\n",
       " 'scouts',\n",
       " ',',\n",
       " '<unk>',\n",
       " ',',\n",
       " 'engineers',\n",
       " ',',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'armored',\n",
       " 'soldier',\n",
       " '.',\n",
       " '<unk>',\n",
       " 'can',\n",
       " 'switch',\n",
       " 'classes',\n",
       " 'by',\n",
       " 'changing',\n",
       " 'their',\n",
       " 'assigned',\n",
       " 'weapon',\n",
       " '.',\n",
       " 'changing',\n",
       " 'class',\n",
       " 'does',\n",
       " 'not',\n",
       " 'greatly',\n",
       " 'affect',\n",
       " 'the',\n",
       " 'stats',\n",
       " 'gained',\n",
       " 'while',\n",
       " 'in',\n",
       " 'a',\n",
       " 'previous',\n",
       " 'class',\n",
       " '.',\n",
       " 'with',\n",
       " 'victory',\n",
       " 'in',\n",
       " 'battle',\n",
       " ',',\n",
       " 'experience',\n",
       " 'points',\n",
       " 'are',\n",
       " 'awarded',\n",
       " 'to',\n",
       " 'the',\n",
       " 'squad',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'distributed',\n",
       " 'into',\n",
       " 'five',\n",
       " 'different',\n",
       " 'attributes',\n",
       " 'shared',\n",
       " 'by',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'squad',\n",
       " ',',\n",
       " 'a',\n",
       " 'feature',\n",
       " 'differing',\n",
       " 'from',\n",
       " 'early',\n",
       " 'games',\n",
       " \"'\",\n",
       " 'method',\n",
       " 'of',\n",
       " 'distributing',\n",
       " 'to',\n",
       " 'different',\n",
       " 'unit',\n",
       " 'types',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '=',\n",
       " '=',\n",
       " 'plot',\n",
       " '=',\n",
       " '=',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'game',\n",
       " 'takes',\n",
       " 'place',\n",
       " 'during',\n",
       " 'the',\n",
       " 'second',\n",
       " 'europan',\n",
       " 'war',\n",
       " '.',\n",
       " 'gallian',\n",
       " 'army',\n",
       " 'squad',\n",
       " '422',\n",
       " ',',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'nameless',\n",
       " ',',\n",
       " 'are',\n",
       " 'a',\n",
       " 'penal',\n",
       " 'military',\n",
       " 'unit',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'criminals',\n",
       " ',',\n",
       " 'foreign',\n",
       " '<unk>',\n",
       " ',',\n",
       " 'and',\n",
       " 'military',\n",
       " 'offenders',\n",
       " 'whose',\n",
       " 'real',\n",
       " 'names',\n",
       " 'are',\n",
       " 'erased',\n",
       " 'from',\n",
       " 'the',\n",
       " 'records',\n",
       " 'and',\n",
       " '<unk>',\n",
       " 'officially',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'by',\n",
       " 'numbers',\n",
       " '.',\n",
       " '<unk>',\n",
       " 'by',\n",
       " 'the',\n",
       " 'gallian',\n",
       " 'military',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'the',\n",
       " 'most',\n",
       " 'dangerous',\n",
       " 'missions',\n",
       " 'that',\n",
       " 'the',\n",
       " 'regular',\n",
       " 'army',\n",
       " 'and',\n",
       " 'militia',\n",
       " 'will',\n",
       " 'not',\n",
       " 'do',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt.examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsilvei/.pyenv/versions/bayesian/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 476.60 | loss  7.99 | ppl  2963.22\n",
      "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 447.33 | loss  6.74 | ppl   846.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-69255d737ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4d7151636315>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bayesian/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/bayesian/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Cyper Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "2. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "3. [Transformers from Scratch](http://www.peterbloem.nl/blog/transformers)\n",
    "4. [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
