{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [1 - 3 Steps of Bayesian Data Analises](#3steps-BDA)\n",
    "* [2 - Bayes Formula - An Intuition](#bayes-formula)\n",
    "* [3 - Most Used Distributions](#distributions)\n",
    "* [4 - Posterior Approximation - A glimpse into MCMC](#mcmc)\n",
    "* [5 - Quick Example in PyMC3](#example)\n",
    "* [6 - Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The three steps of Bayesian data analysis (Gelman) <a class=\"anchor\" id=\"3steps-BDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequentist Approach\n",
    "\n",
    "- Need robust calibration\n",
    "\n",
    "\n",
    "#### Bayesian Inference\n",
    "- Model not only frequency but also epistemological variation and weights over the model configuration space  \n",
    "- A probability distribution encodes much more information than a step function!\n",
    "\n",
    " - **Prior** = prior distribution encodes domain expertise about the model configurations in the observational model, and possibly even the context of the observational model relative to the true data generating process and latent observational process  \n",
    "  - prior distributions do not need to encode all of our domain expertise but rather just enough to ensure useful inferences  \n",
    " - **Likelihood** = The likelihood function maps each model configuration to a numerical quantification that increases for model configurations that are more consistent with the specific observation and decreases for those model configurations that are less consistent. In other words, the likelihood function quantifies the relative consistency of each model configuration with the observed data.  \n",
    " - **Posterior** = Bayes‚Äô Theorem can be thought of as switching from one conditional probability density function, that specifying the observational model, to another, that specifying the posterior distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contraction** $\\rightarrow$ likelihood function is more informative than, but also consistent with, the prior distribution:\n",
    "\n",
    " <img src=\"imgs_prez/img7.png\" width=\"480\" height=\"480\" align=\"center\"/>\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Containment** $\\rightarrow$ prior distribution is more informative than, but also consistent with, the likelihood function:\n",
    "\n",
    " <img src=\"imgs_prez/img8.png\" width=\"480\" height=\"480\" align=\"center\"/>\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compromise** $\\rightarrow$ when there is tension between the information encoded in the likelihood function and the prior:\n",
    "\n",
    " <img src=\"imgs_prez/img9.png\" width=\"480\" height=\"480\" align=\"center\"/>\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"imgs_prez/img1.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"imgs_prez/img2.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    " \n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs_prez/img3.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    "\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs_prez/img4.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    "\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs_prez/img5.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    "\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs_prez/img6.png\" width=\"640\" height=\"640\" align=\"center\"/>\n",
    "\n",
    " **Source**: betanalpha.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayes Formula - An Intuition <a class=\"anchor\" id=\"bayes-formula\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From product rule we have\n",
    "\n",
    "$$\n",
    "p(\\theta,  y)= p(\\theta \\mid y) \\: p(y) \n",
    "$$\n",
    "\n",
    "Can be also written as:\n",
    "$$\n",
    "p(\\theta,  y)= p(y \\mid \\theta ) \\: p(\\theta)\n",
    "$$\n",
    "\n",
    "Re-ordening\n",
    "$$\n",
    "p(\\theta \\mid y) \\: p(y) = p(y \\mid \\theta ) \\: p(\\theta)\n",
    "$$\n",
    "\n",
    "Finally üòç\n",
    "$$\n",
    "p(\\theta \\mid y)  = \\frac{p(y \\mid \\theta ) \\: p(\\theta)}{p(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where:\n",
    "    \n",
    "$p(y \\mid \\theta ) \\rightarrow$ **Likelihood** =\"plausibility\"of the data given the parameters  \n",
    "\n",
    "$p(\\theta)  \\rightarrow$ **Prior Distribution** = What we know about parameters withouth seen the data \n",
    "\n",
    "$p(y) \\rightarrow $  **Marginal Likelihood or Evidence** =\n",
    "\n",
    "$(\\theta \\mid y) \\rightarrow $  **Posterior Distribution** = compromise between prior and likelihood, updating prior believes in light of new data $\\rightarrow$ suitable for **sequential** data analysis  \n",
    "\n",
    "Writting differently ($ \\theta = hypothesis $, $ y = data $):   \n",
    "$$\n",
    "p(hypothesis \\mid data)  = \\frac{p(data \\mid hypothesis ) \\: p(hypothesis)}{p(data)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### _\"A Bayesian us one who, vagely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Most Used Distributions <a class=\"anchor\" id=\"distributions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Continuous Distributions** <a class=\"anchor\" id=\"continuous\"></a>  \n",
    "1.1 - Normal (Gaussian) <a class=\"anchor\" id=\"normal\"></a>  \n",
    "1.2 - T-Student (Robust Inference) <a class=\"anchor\" id=\"tstudent\"></a>  \n",
    "1.3 - Beta (univariate)  <a class=\"anchor\" id=\"beta\"></a>  \n",
    "1.4 - Dirichlet (multivariate)  <a class=\"anchor\" id=\"dirichlet\">  </a>\n",
    "1.5 - Gamma  <a class=\"anchor\" id=\"gamma\"></a>  \n",
    "1.6 - Uniform  \n",
    "\n",
    "2. **Discrete Distributions**  \n",
    "2.1 - Bernouli  \n",
    "2.2 - Binomial\n",
    "2.3 - Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior Approximation - A glimpse into MCMC <a class=\"anchor\" id=\"mcmc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- While convalescing from an illness in 1946, Stan Ulam was playing solitaire. It, then, occurred to him to try to compute the chances that a particular solitaire laid out with 52 cards would come out successfully (Eckhard, 1987). After attempting exhaustive combinatorial calculations, he decided to go for the more practical approach of laying out several solitaires at random and then observing and counting the number of successful plays. This idea of selecting a statistical sample to approximate a hard combinatorial problem by a much simpler problem is at the heart of modern Monte Carlo simulation.\n",
    "- MCMC algorithms typically require the design of proposal mechanisms to generate candidate hypotheses\n",
    "- MCMC techniques are often applied to solve integration and optimisation problems in large dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrico_fermi.jpeg img3.png          img6.png          img9.png\r\n",
      "img1.png          img4.png          img7.png          stan_ulam.jpeg\r\n",
      "img2.png          img5.png          img8.png          von_neumann.jpeg\r\n"
     ]
    }
   ],
   "source": [
    "!ls imgs_prez/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs_prez/stan_ulam.jpeg\" width=\"240\" height=\"240\" align=\"center\"/>\n",
    "<img src=\"imgs_prez/von_neumann.jpeg\" width=\"240\" height=\"240\" align=\"center\"/>\n",
    "<img src=\"imgs_prez/enrico_fermi.jpeg\" width=\"240\" height=\"240\" align=\"center\"/>\n",
    " **Source**: An Introduction to MCMC for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Monte Carlo principle\n",
    "\n",
    "$$\n",
    "p_{N}(x)=\\frac{1}{N}\\sum_{i=1}^{N} \\delta_{x}^{(i)}(x)\n",
    "$$\n",
    "\n",
    "Approximate integrals (or very large sums) $I(f)$ with tractable sums $I_{N}(f)$\n",
    "\n",
    "\n",
    "$$\n",
    "I_{N}(f)=\\frac{1}{N}\\sum_{i=1}^{N} f(x^{(i)}) \\xrightarrow[\\inf]{\\text{a.s.}}I(f) = \\int_{x} f(x)p(x)d(x)\n",
    "$$\n",
    "\n",
    "### Metropolis-Hastings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Nando MCMC Videos\n",
    "- Video 1: https://youtu.be/TNZk8lo4e-Q\n",
    "- Video 2: https://youtu.be/sK3cg15g8FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Example in PyMC3  <a class=\"anchor\" id=\"example\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources <a class=\"anchor\" id=\"sources\"></a>\n",
    "\n",
    "1. [Michael Betancourt - Probabilistic Modeling and Statistical Inference](https://betanalpha.github.io/assets/case_studies/modeling_and_inference.html)  \n",
    "2. [Michael Betancourt - Probabilistic Building Blocks](https://betanalpha.github.io/assets/case_studies/probability_densities.html)  \n",
    "3. [Michael Betancourt - Markov Chain Monte Carlo in Practice](https://betanalpha.github.io/assets/case_studies/markov_chain_monte_carlo.html)  \n",
    "4. [Aerin Kim - Bayesian Inference ‚Äî Intuition and Example](https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6)\n",
    "5. [AllenDowney - ThinkBayes2](https://github.com/AllenDowney/ThinkBayes2)\n",
    "6. [Andrew Gelman - Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/)  \n",
    "7. [Colin Carroll - imcmc](https://github.com/ColCarroll/imcmc)\n",
    "8. [Andrieu, Freitas - An Introduction to MCMC for Machine Learning](https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf)  \n",
    "9. [Eric Ma - An Introduction to Probability and Computational Bayesian Statistics](https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
