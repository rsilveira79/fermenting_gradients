{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "# Torch, Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "print(torch.__version__)\n",
    "\n",
    "## Embeddings\n",
    "import allennlp\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## NLP libs\n",
    "from nltk import download\n",
    "import gensim\n",
    "\n",
    "stopwords = {\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m2017-06-custom-intent-engines\u001b[m\u001b[m\r\n",
      "README.md\r\n",
      "Untitled.ipynb\r\n",
      "intent_classifier_uncertainty_mc_dropout.ipynb\r\n",
      "model__uncertainty.pth\r\n",
      "snips_dataset.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../personal/intents_uncertainty/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../../personal/intents_uncertainty/2017-06-custom-intent-engines/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.DataFrame(columns = ['phrase', 'intent'])\n",
    "# for intent in ['AddToPlaylist', 'BookRestaurant', 'GetWeather', 'PlayMusic', 'RateBook', 'SearchCreativeWork',\n",
    "#                'SearchScreeningEvent']:\n",
    "#     with open(path_dataset + intent + \"/train_\" + intent + \".json\",\n",
    "#               encoding='cp1251') as data_file:\n",
    "#         data = json.load(data_file)\n",
    "#     print(\"Intent: {}, Length: {}\".format(intent,len(data[intent])))\n",
    "#     texts = []\n",
    "#     for i in range(len(data[intent])):\n",
    "#         text = ''\n",
    "#         for j in range(len(data[intent][i]['data'])):\n",
    "#             text += data[intent][i]['data'][j]['text']\n",
    "#         dataset = dataset.append({'phrase': text, 'intent': intent}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>modify her military information</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>modify worker military status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>change employee veteran status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>change his military status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>update Brian's veteran status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               phrase                      intent\n",
       "2671  modify her military information  workerVeteranStatus.update\n",
       "2672    modify worker military status  workerVeteranStatus.update\n",
       "2673   change employee veteran status  workerVeteranStatus.update\n",
       "2674       change his military status  workerVeteranStatus.update\n",
       "2675    update Brian's veteran status  workerVeteranStatus.update"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle('intents_phrases_183.pkl')\n",
    "dataset = dataset.rename(columns={\"usersays\":\"phrase\"})\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text, do_stop=False, do_stem=False):\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Cleaning input\n",
    "    text = text.replace(\"'s\",\"\")\n",
    "    text = text.replace(\"â€™s\",\"\")\n",
    "    text = text.replace(\"?\",\"\")\n",
    "    text = text.replace(\"-\",\"\")\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    # Removing all the stopwords\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stopwords]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    if (do_stem==True):\n",
    "        # Stemming\n",
    "        text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>intent</th>\n",
       "      <th>preproc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>modify her military information</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "      <td>modify military information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>modify worker military status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "      <td>modify worker military status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>change employee veteran status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "      <td>change employee veteran status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>change his military status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "      <td>change military status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>update Brian's veteran status</td>\n",
       "      <td>workerVeteranStatus.update</td>\n",
       "      <td>update brian veteran status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               phrase                      intent  \\\n",
       "2671  modify her military information  workerVeteranStatus.update   \n",
       "2672    modify worker military status  workerVeteranStatus.update   \n",
       "2673   change employee veteran status  workerVeteranStatus.update   \n",
       "2674       change his military status  workerVeteranStatus.update   \n",
       "2675    update Brian's veteran status  workerVeteranStatus.update   \n",
       "\n",
       "                        preproc_text  \n",
       "2671     modify military information  \n",
       "2672   modify worker military status  \n",
       "2673  change employee veteran status  \n",
       "2674          change military status  \n",
       "2675     update brian veteran status  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['preproc_text'] = dataset['phrase'].apply(lambda x: transformText(x, do_stop=True))\n",
    "dataset.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dicionario: 792\n"
     ]
    }
   ],
   "source": [
    "## Build word vocabulary\n",
    "word_to_ix = {}\n",
    "for sent in dataset.preproc_text:\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(\"Tamanho do dicionario: {}\".format(len(word_to_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Labels: 183\n"
     ]
    }
   ],
   "source": [
    "## Build label vocabulary\n",
    "label_to_ix = {}\n",
    "for label in dataset.intent:\n",
    "    for word in label.split():\n",
    "        if word not in label_to_ix:\n",
    "            label_to_ix[word]=len(label_to_ix)\n",
    "print(\"# Labels: {}\".format(len(label_to_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intents(Dataset):\n",
    "    def __init__(self, dataframe, w2v_weights_path):\n",
    "        self.len = len(dataframe)\n",
    "        self.label_to_ix = {}\n",
    "        self.data = dataframe\n",
    "        self.w2v = KeyedVectors.load_word2vec_format(w2v_weights_path, binary = True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        phrase = self.data.preproc_text[index]\n",
    "        X, _  = self.get_avg_sentence_vector(phrase)\n",
    "        y = label_to_ix[self.data.intent[index]]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def get_avg_sentence_vector(self, sentence):\n",
    "        featureVec = np.zeros((self.w2v.vector_size), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "        not_found_words = []\n",
    "        for word in sentence.split():\n",
    "            if word in self.w2v.index2word:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, self.w2v.get_vector(word))\n",
    "            else:\n",
    "                not_found_words.append(word)\n",
    "        if nwords>0:\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec, not_found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_weights_path = '../../../vectors/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=dataset.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
    "test_dataset=dataset.drop(train_dataset.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2676, 3)\n",
      "TRAIN Dataset: (2141, 3)\n",
      "TEST Dataset: (535, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL Dataset: {}\".format(dataset.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Intents(train_dataset,  w2v_weights_path)\n",
    "testing_set = Intents(test_dataset, w2v_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 16,\n",
    "          'shuffle': True,\n",
    "          'drop_last': True,\n",
    "          'num_workers': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, **params)\n",
    "testing_loader = DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x4861b4390>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_phrases, sample_labels = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 300])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_phrases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = torch.zeros((0,300))\n",
    "# train_y = torch.zeros(0, dtype=torch.int64)\n",
    "# for i, (x, y)  in enumerate(training_loader):\n",
    "#     print(\"--- \", i)\n",
    "#     train_x = torch.cat((train_x,x),0)\n",
    "#     train_y = torch.cat((train_y,y),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2112, 300])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, inputdim, \n",
    "                        nclasses, \n",
    "                        nhidden, \n",
    "                        dropout = 0,\n",
    "                        cudaEfficient=True):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        self.inputdim = inputdim\n",
    "        self.hidden_dim = nhidden\n",
    "        self.dropout = dropout\n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        if cudaEfficient:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, nhidden),\n",
    "                nn.Dropout(p=self.dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(nhidden, self.nclasses),\n",
    "                ).cuda()\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, nhidden),\n",
    "                nn.Dropout(p=self.dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(nhidden, self.nclasses),\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        log_probs = self.model(x)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIM = training_set.w2v.vector_size\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "NHIDDEN = 512\n",
    "DROPOUT = 0.3\n",
    "model = SimpleMLP(inputdim = INP_DIM ,\n",
    "          nhidden = NHIDDEN,\n",
    "          nclasses = NUM_LABELS,\n",
    "          dropout = DROPOUT, \n",
    "          cudaEfficient = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)\n",
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, label = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 300]),\n",
       " tensor([ 26,  54, 112, 156, 122, 172,  68, 149,  85, 155,  47, 178,  19, 116,\n",
       "         162,   0]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 183])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0365,  0.0076, -0.0119,  ..., -0.0663, -0.0285, -0.0482],\n",
       "        [ 0.0118,  0.0093, -0.0086,  ..., -0.0373, -0.0271, -0.0134],\n",
       "        [-0.0038,  0.0147, -0.0383,  ..., -0.0472,  0.0040, -0.0483],\n",
       "        ...,\n",
       "        [-0.0170,  0.0533,  0.0013,  ..., -0.0389, -0.0172, -0.0678],\n",
       "        [ 0.0336,  0.0031, -0.0378,  ..., -0.0263,  0.0331, -0.0677],\n",
       "        [ 0.0202,  0.0667, -0.0040,  ..., -0.0411,  0.0082, -0.0644]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(sent)\n",
    "print(out.shape)\n",
    "print(label.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2124, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(out, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH -- 0\n",
      "Iteration: 0. Loss: 5.208669185638428. Accuracy: 0.1953125%\n",
      "EPOCH -- 1\n",
      "Iteration: 0. Loss: 4.407190322875977. Accuracy: 10.9375%\n",
      "EPOCH -- 2\n",
      "Iteration: 0. Loss: 3.0471835136413574. Accuracy: 33.3984375%\n",
      "EPOCH -- 3\n",
      "Iteration: 0. Loss: 2.327571392059326. Accuracy: 52.34375%\n",
      "EPOCH -- 4\n",
      "Iteration: 0. Loss: 1.5701735019683838. Accuracy: 63.4765625%\n"
     ]
    }
   ],
   "source": [
    "def train(model, epochs):\n",
    "    max_epochs = epochs\n",
    "    model = model.train()\n",
    "    for epoch in tqdm_notebook(range(max_epochs)):\n",
    "        print(\"EPOCH -- {}\".format(epoch))\n",
    "        for i, (sent, labels) in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(sent)\n",
    "            loss = loss_function(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i%500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            output = model.forward(ids.cuda(), token_type_ids=tokens.cuda(), head_mask=None)[0]\n",
    "                  _, predicted = torch.max(output.data, 1)\n",
    "                  total += labels.size(0)\n",
    "                  correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "              accuracy = 100.00 * correct.numpy() / total\n",
    "              print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))\n",
    "    return \"Training finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.utils.deprecation import _deprecate_kwarg_with_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLikelihood2(Likelihood):\n",
    "    \"\"\"\n",
    "    Implements the Softmax (multiclass) likelihood used for GP classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_features=None, num_classes=None, mixing_weights=True, mixing_weights_prior=None, **kwargs\n",
    "    ):\n",
    "        num_classes = _deprecate_kwarg_with_transform(\n",
    "            kwargs, \"n_classes\", \"num_classes\", num_classes, lambda n: n\n",
    "        )\n",
    "        super().__init__()\n",
    "        if num_classes is None:\n",
    "            raise ValueError(\"num_classes is required\")\n",
    "        self.num_classes = num_classes\n",
    "        if mixing_weights:\n",
    "            self.num_features = num_features\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"num_features is required with mixing weights\")\n",
    "            self.register_parameter(\n",
    "                name=\"mixing_weights\",\n",
    "                parameter=torch.nn.Parameter(torch.randn(num_classes, num_features).div_(num_features)),\n",
    "            )\n",
    "            if mixing_weights_prior is not None:\n",
    "                self.register_prior(\"mixing_weights_prior\", mixing_weights_prior, \"mixing_weights\")\n",
    "        else:\n",
    "            self.num_features = num_classes\n",
    "            self.mixing_weights = None\n",
    "        print(self.num_features)\n",
    "\n",
    "    def forward(self, function_samples, *params, **kwargs):\n",
    "        num_features, num_data = function_samples.shape[-2:]\n",
    "        if num_features != self.num_features:\n",
    "            raise RuntimeError(\"There should be %d features\" % self.num_features)\n",
    "\n",
    "        if self.mixing_weights is not None:\n",
    "            mixed_fs = self.mixing_weights @ function_samples  # num_classes x num_data\n",
    "        else:\n",
    "            mixed_fs = function_samples\n",
    "        mixed_fs = mixed_fs.transpose(-1, -2)  # num_data x num_classes\n",
    "        res = base_distributions.Categorical(logits=mixed_fs)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "\n",
    "class GPClassificationModel(AbstractVariationalGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(train_x)\n",
    "#likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "likelihood = SoftmaxLikelihood2(num_features=300,\n",
    "                                mixing_weights=True,\n",
    "                                num_classes=len(label_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Parameters\n",
    "learning_rate = 0.1\n",
    "max_epochs = 10\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mll = VariationalELBO(likelihood, model, train_y.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(train_x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-mll(output, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    for i, (sent, label) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(sent)        \n",
    "        loss = -mll(output, label)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i, epoch, loss.item()))\n",
    "        \n",
    "#         if i%100 == 0:\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             for sent, label in testing_loader:\n",
    "#                 sent = Variable(sent)\n",
    "#                 label = Variable(label)\n",
    "#                 output = model.forward(sent)\n",
    "#                 _, predicted = torch.max(output.mean, 1)\n",
    "#                 total += label.size(0)\n",
    "#                 correct += (predicted.cpu() == label.cpu()).sum()\n",
    "#             accuracy = 100.00 * correct.numpy() / total\n",
    "#             print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into eval mode\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "    # Get classification predictions\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_phrase = \"i need to book a restaurant today\"\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "inp, _ = training_set.get_avg_sentence_vector(input_phrase)\n",
    "inp = torch.tensor(inp)\n",
    "observed_pred = likelihood(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reply(phrase):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    inp, _ = training_set.get_avg_sentence_vector(phrase)\n",
    "    inp = torch.Tensor(inp)\n",
    "    output = model.forward(inp)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "#     # Get predictions from the maximum value\n",
    "#     _, predicted = torch.max(output.data, 0)\n",
    "#     pred_label=list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted.item())]\n",
    "#     return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reply(input_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sources\n",
    "https://gpytorch.readthedocs.io/en/latest/examples/08_Deep_Kernel_Learning/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
