{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numeric-mouse",
   "metadata": {},
   "source": [
    "#### Paper - On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\n",
    "- Thulasidasan et al\n",
    "- NeurIPS 2019\n",
    "\n",
    "\n",
    "_Mixup_ Training\n",
    "\n",
    "$$\n",
    "\\tilde{x} = \\lambda x_{i} + (1-\\lambda)x_{j}\\newline\n",
    "\\tilde{y} = \\lambda y_{i} + (1-\\lambda)y_{j}\\newline\n",
    "\\text{Where:} \\newline\n",
    "\\lambda \\in [0,1] \\sim Beta(\\alpha,\\alpha)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-macintosh",
   "metadata": {},
   "source": [
    "#### Paper - On Calibration of Modern Neural Networks\n",
    "- Guo et al\n",
    "- ICML 2017\n",
    "\n",
    "$$\n",
    "X \\in \\mathcal{X} \\hspace{5mm} (\\text{inputs}) \\newline\n",
    "Y \\in \\mathcal{Y} = \\{1, .., K\\} \\hspace{5mm} (\\text{labels})\\newline\n",
    "h(x) = (\\hat{Y}, \\hat{P})   \\hspace{5mm} (\\hat{Y} = \\text{predictions}, \\hat{Y} = \\text{confidences})\\newline\n",
    "\\mathbb{P}(\\hat{Y} = Y | \\hat{P}=p)=p, \\; \\forall p \\in [0,1] \\hspace{5mm} (\\text{Perfect calibration})\\newline\n",
    "$$  \n",
    "\n",
    "\n",
    "**ECE**\n",
    "$$\n",
    "ECE = \\sum_{m=1}^{M}\\frac{|B_{m}|}{n}|acc(B_{m})-conf(B_{m})| \\newline\n",
    "$$\n",
    "\n",
    "**MCE**\n",
    "$$\n",
    "MCE = \\max_{m\\in\\{1,...,M\\}} |acc(B_{m})-conf(B_{m})| \\newline\n",
    "$$\n",
    "\n",
    "**Platt Scaling**\n",
    "$$\n",
    "P(y=1|f)=\\frac{1}{1+e^{Af+B}}\n",
    "$$\n",
    "\n",
    "**Isotonic Regression**\n",
    "$$\n",
    "y_{i}=m(f_{i}) + \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}=argmin_{z}\\sum(y_{i}-z(f_{i}))^2\n",
    "$$\n",
    "\n",
    "**Temperature Scaling**\n",
    "\n",
    "$$\n",
    "\\hat{q_{i}} = \\max_{k} \\sigma_{SM} (z_{i}/T)^{k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-aberdeen",
   "metadata": {},
   "source": [
    "#### Paper - Obtaining Well Calibrated Probabilities Using Bayesian Binning\n",
    "- Naeini et al\n",
    "- AAAI 2015\n",
    "- Bayesian Binning into Quantiles (BBQ)\n",
    "- post processes the output of a binary classification algorithm\n",
    "- In all these the post-processing step can be seen as a function that maps the output of a prediction model to probabilities that are intended to be well-calibrated\n",
    "- BBQ extends the simple histogram-binning calibration method by considering multiple binning models and their combination\n",
    "- Distribute the data-points in the training set equally across all bins\n",
    "\n",
    "\n",
    "Score\n",
    "$$\n",
    "Score(M) = P(M).P(\\mathcal{D}|M) \\newline\n",
    "\\text{Where:} \\newline\n",
    "P(\\mathcal{D}|M) = \\prod_{b=1}^B \\frac{\\Gamma(\\frac{N'}{B})}{\\Gamma(N_{b}+\\frac{N'}{B})}\n",
    "\\frac{\\Gamma(m_{b}+\\alpha_{b})}{\\Gamma(\\alpha_{b})}\n",
    "\\frac{\\Gamma(n_{b}+\\beta_{b})}{\\Gamma(\\beta_{b})}\\newline\n",
    "P(M) = \\text{Prior = Uniform Distribution}\n",
    "$$\n",
    "\n",
    "Calibrated Predictions\n",
    "\n",
    "$$\n",
    "P(z=1|y) = \\sum_{i=1}^{T} \\frac{Score(M_{i})}{\\sum_{j=1}^{T}Score(M_{j})}P(z=1|y, M_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-renewal",
   "metadata": {},
   "source": [
    "#### Paper - Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings\n",
    "- Kumar et al\n",
    "- ICML 2018\n",
    "- a practical and principled fix by minimizing calibration error during training along with classification error\n",
    "- Unfortunately on high capacity neural networks, NLL fails to minimize calibration error because of over-fitting\n",
    "\n",
    "\n",
    "Trainable function that encompases Accuracy and Calibration error\n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\; NLL(D,\\theta) + \\alpha \\; CE(D,\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "MMCE_{m}(D) = \\left\\| \\sum_{(r_{i},c_{i})\\in D} \\frac{(c_{i}-r_{i})\\; \\phi(r_{i})}{m} \\right\\|_\\mathcal{H}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-disability",
   "metadata": {},
   "source": [
    "#### Paper - Regularizing Neural Networks by Penalizing Confident Output Distributions\n",
    "- Pereyra et al (w/ Hinton and Lukasz Kaiser - Transformer)\n",
    "- ICLR 2017\n",
    "\n",
    "- Label smoothing + Confidence penalization\n",
    "\n",
    "\n",
    "- probabilities assigned to class labels that are incorrect (according to the training data) are part of the knowledge of the network\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\sum \\log p_{\\theta}(y|x) - \\beta \\; H(p_{\\theta}(y|x))\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "H(p_{\\theta} (y|x)) = - \\sum_{i} p_{\\theta}(y_{i}|x) \\; \\log(p_{\\theta}(y_{i}|x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-smart",
   "metadata": {},
   "source": [
    "#### Paper - Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration\n",
    "- Kull et al\n",
    "- NeurIPS 2019\n",
    "\n",
    "\n",
    "\n",
    "**Claswise-ECE**\n",
    "$$\n",
    "classwise-ECE = \\frac{1}{k} \\sum_{j=1}^{k} \\sum_{i=1}^{m}\\frac{|B_{i,j}|}{n}|acc(B_{i,j})-conf(B_{i,j})| \\newline\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-serbia",
   "metadata": {},
   "source": [
    "#### Paper - Brier Score\n",
    "\n",
    "\n",
    "Brier Score\n",
    "\n",
    "$$\n",
    "BS = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}(y_{i}-p_{i})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-currency",
   "metadata": {},
   "source": [
    "#### Paper - Calibrating Deep Neural Networks using Focal Loss\n",
    "- Mukhoti et al\n",
    "- NeurIPS 2020\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{f}=-(1-\\hat{p}_{i,y_{i}})^\\gamma \\log \\hat{p}_{i,y_{i}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{f} \\geq KL(q||\\hat{p}) - \\gamma \\mathbb{H}[\\hat{p}]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-blast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-green",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-lying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "possible-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-broadcast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03749999999999999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "going-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03749999999999999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "funky-guess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true, 1-y_prob, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indian-china",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true, 1-y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-tiger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
