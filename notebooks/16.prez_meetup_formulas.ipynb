{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorporate-royalty",
   "metadata": {},
   "source": [
    "#### Paper - On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\n",
    "- Thulasidasan et al\n",
    "- NeurIPS 2019\n",
    "\n",
    "\n",
    "_Mixup_ Training\n",
    "\n",
    "$$\n",
    "\\tilde{x} = \\lambda x_{i} + (1-\\lambda)x_{j}\\newline\n",
    "\\tilde{y} = \\lambda y_{i} + (1-\\lambda)y_{j}\\newline\n",
    "\\text{Where:} \\newline\n",
    "\\lambda \\in [0,1] \\sim Beta(\\alpha,\\alpha)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-macintosh",
   "metadata": {},
   "source": [
    "#### Paper - On Calibration of Modern Neural Networks\n",
    "- Guo et al\n",
    "- ICML 2017\n",
    "\n",
    "$$\n",
    "X \\in \\mathcal{X} \\hspace{5mm} (\\text{inputs}) \\newline\n",
    "Y \\in \\mathcal{Y} = \\{1, .., K\\} \\hspace{5mm} (\\text{labels})\\newline\n",
    "h(x) = (\\hat{Y}, \\hat{P})   \\hspace{5mm} (\\hat{Y} = \\text{predictions}, \\hat{Y} = \\text{confidences})\\newline\n",
    "\\mathbb{P}(\\hat{Y} = Y | \\hat{P}=p)=p, \\; \\forall p \\in [0,1] \\hspace{5mm} (\\text{Perfect calibration})\\newline\n",
    "$$  \n",
    "\n",
    "\n",
    "**ECE**\n",
    "$$\n",
    "ECE = \\sum_{m=1}^{M}\\frac{|B_{m}|}{n}|acc(B_{m})-conf(B_{m})| \\newline\n",
    "$$\n",
    "\n",
    "**MCE**\n",
    "$$\n",
    "MCE = \\max_{m\\in\\{1,...,M\\}} |acc(B_{m})-conf(B_{m})| \\newline\n",
    "$$\n",
    "\n",
    "**Platt Scaling**\n",
    "$$\n",
    "P(y=1|f)=\\frac{1}{1+e^{Af+B}}\n",
    "$$\n",
    "\n",
    "**Isotonic Regression**\n",
    "$$\n",
    "y_{i}=m(f_{i}) + \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}=argmin_{z}\\sum(y_{i}-z(f_{i}))^2\n",
    "$$\n",
    "\n",
    "**Temperature Scaling**\n",
    "\n",
    "$$\n",
    "\\hat{q_{i}} = \\max_{k} \\sigma_{SM} (z_{i}/T)^{k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-aberdeen",
   "metadata": {},
   "source": [
    "#### Paper - Obtaining Well Calibrated Probabilities Using Bayesian Binning\n",
    "- Naeini et al\n",
    "- AAAI 2015\n",
    "- Bayesian Binning into Quantiles (BBQ)\n",
    "- post processes the output of a binary classification algorithm\n",
    "- In all these the post-processing step can be seen as a function that maps the output of a prediction model to probabilities that are intended to be well-calibrated\n",
    "- BBQ extends the simple histogram-binning calibration method by considering multiple binning models and their combination\n",
    "- Distribute the data-points in the training set equally across all bins\n",
    "\n",
    "\n",
    "Score\n",
    "$$\n",
    "Score(M) = P(M).P(\\mathcal{D}|M) \\newline\n",
    "\\text{Where:} \\newline\n",
    "P(\\mathcal{D}|M) = \\prod_{b=1}^B \\frac{\\Gamma(\\frac{N'}{B})}{\\Gamma(N_{b}+\\frac{N'}{B})}\n",
    "\\frac{\\Gamma(m_{b}+\\alpha_{b})}{\\Gamma(\\alpha_{b})}\n",
    "\\frac{\\Gamma(n_{b}+\\beta_{b})}{\\Gamma(\\beta_{b})}\\newline\n",
    "P(M) = \\text{Prior = Uniform Distribution}\n",
    "$$\n",
    "\n",
    "Calibrated Predictions\n",
    "\n",
    "$$\n",
    "P(z=1|y) = \\sum_{i=1}^{T} \\frac{Score(M_{i})}{\\sum_{j=1}^{T}Score(M_{j})}P(z=1|y, M_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-renewal",
   "metadata": {},
   "source": [
    "#### Paper - Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings\n",
    "- Kumar et al\n",
    "- ICML 2018\n",
    "- a practical and principled fix by minimizing calibration error during training along with classification error\n",
    "- Unfortunately on high capacity neural networks, NLL fails to minimize calibration error because of over-fitting\n",
    "\n",
    "\n",
    "Trainable function that encompases Accuracy and Calibration error\n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\; NLL(D,\\theta) + \\alpha \\; CE(D,\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "MMCE_{m}(D) = \\left\\| \\sum_{(r_{i},c_{i})\\in D} \\frac{(c_{i}-r_{i})\\; \\phi(r_{i})}{m} \\right\\|_\\mathcal{H}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-disability",
   "metadata": {},
   "source": [
    "#### Paper - Regularizing Neural Networks by Penalizing Confident Output Distributions\n",
    "- Pereyra et al (w/ Hinton and Lukasz Kaiser - Transformer)\n",
    "- ICLR 2017\n",
    "\n",
    "- Label smoothing + Confidence penalization\n",
    "\n",
    "\n",
    "- probabilities assigned to class labels that are incorrect (according to the training data) are part of the knowledge of the network\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\sum \\log p_{\\theta}(y|x) - \\beta \\; H(p_{\\theta}(y|x))\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "H(p_{\\theta} (y|x)) = - \\sum_{i} p_{\\theta}(y_{i}|x) \\; \\log(p_{\\theta}(y_{i}|x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-growing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
